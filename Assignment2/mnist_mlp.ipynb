{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the MNIST digits classification dataset (grey images), and the code provided, we get a test accuracy of 0.9825000166893005. \n",
    "Removing the dropout layer can lead to deregularization. I use it, but did not seem to have a great effect on the final result (0.9822999835014343).\n",
    "After this I decided to change the Dropout value from 0.2 to 0.5 to check if it has a great effect on the performance. With this the performance got a slightly bit better (0.9837999939918518), so I decided to use 0.7 and got a worst one (around 97). So I was wondered if we really needed all the layers that we have. I decided to remove the hidden layer and got a similar value (around 98%) even without the dropout layer. This problem seems quite easy to be solve for neural network. Compared to original one, if we change the activation function from relu to sigmoid. It took long to converge and we got 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "11501568/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_classes) # each instance is an array \n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 8s 15ms/step - loss: 0.2171 - accuracy: 0.9335 - val_loss: 0.1159 - val_accuracy: 0.9631\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0812 - accuracy: 0.9748 - val_loss: 0.0756 - val_accuracy: 0.9765\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0539 - accuracy: 0.9827 - val_loss: 0.0785 - val_accuracy: 0.9787\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0384 - accuracy: 0.9883 - val_loss: 0.0848 - val_accuracy: 0.9782\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0288 - accuracy: 0.9909 - val_loss: 0.1237 - val_accuracy: 0.9679\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0228 - accuracy: 0.9929 - val_loss: 0.0809 - val_accuracy: 0.9823\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 8s 16ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 0.0858 - val_accuracy: 0.9836\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0160 - accuracy: 0.9953 - val_loss: 0.1125 - val_accuracy: 0.9783\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.1045 - val_accuracy: 0.9825\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.1161 - val_accuracy: 0.9832\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.1298 - val_accuracy: 0.9804\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.1271 - val_accuracy: 0.9826\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0072 - accuracy: 0.9981 - val_loss: 0.1629 - val_accuracy: 0.9793\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.1361 - val_accuracy: 0.9829\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.1825 - val_accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 7s 15ms/step - loss: 0.0066 - accuracy: 0.9983 - val_loss: 0.1804 - val_accuracy: 0.9776\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.1716 - val_accuracy: 0.9813\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.1498 - val_accuracy: 0.9835\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.1847 - val_accuracy: 0.9821\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 7s 16ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.1807 - val_accuracy: 0.9823\n",
      "Test loss: 0.1807299256324768\n",
      "Test accuracy: 0.9822999835014343\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# the warning message is expected"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
